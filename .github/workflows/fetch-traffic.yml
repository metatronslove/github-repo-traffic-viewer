name: GitHub Traffic Data Collector

on:
  schedule:
    - cron: '0 * * * *'  # Hourly run
  workflow_dispatch:

permissions:
  contents: write

jobs:
  fetch-traffic-data:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.PERSONAL_ACCESS_TOKEN }}
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pygithub==1.59.0 pyyaml

      - name: Initialize Data Directory
        run: |
          mkdir -p docs/data/repos
          echo "DATA_DIR=docs/data" >> $GITHUB_ENV

      - name: Collect and Merge Traffic Data
        env:
          GH_TOKEN: ${{ secrets.PERSONAL_ACCESS_TOKEN }}
        run: |
          python - <<EOF
          from github import Github
          import os
          import json
          import time
          from datetime import datetime, timezone

          print("ğŸš€ Starting traffic data collection...")
          
          g = Github(os.environ['GH_TOKEN'])
          data_dir = os.environ['DATA_DIR']
          
          def parse_traffic_data(api_response, data_type):
              """API'den gelen dictionary yapÄ±sÄ±nÄ± iÅŸle"""
              if not api_response:
                  print(f"    âš ï¸ {data_type}: API boÅŸ dÃ¶ndÃ¼!")
                  return []
              
              # API'den gelen veri bir dictionary
              if isinstance(api_response, dict):
                  if data_type in api_response:
                      items = api_response[data_type]
                      print(f"    ğŸ“Š {data_type}: {len(items)} kayÄ±t geldi")
                      
                      # Ã–rnek kayÄ±tlarÄ± gÃ¶ster
                      for i, item in enumerate(items[:3]):
                          ts = item['timestamp']
                          if isinstance(ts, datetime):
                              ts = ts.astimezone(timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ')
                          print(f"      Ã–rnek {i+1}: {ts} - count: {item['count']}, uniques: {item['uniques']}")
                      
                      return items
                  else:
                      print(f"    âš ï¸ {data_type}: API yanÄ±tÄ±nda '{data_type}' anahtarÄ± yok!")
                      print(f"    API anahtarlarÄ±: {list(api_response.keys())}")
                      return []
              else:
                  print(f"    âš ï¸ {data_type}: Beklenmeyen veri tipi: {type(api_response)}")
                  return []
          
          def merge_data(existing, new_items, data_key):
              # Mevcut veri yoksa yeni yapÄ± oluÅŸtur
              if not existing or data_key not in existing:
                  existing = {data_key: [], "count": 0, "uniques": 0}
                  print(f"    ğŸ“ Yeni {data_key} dosyasÄ± oluÅŸturulacak")
              
              # Mevcut timestamp'leri normalize et
              existing_timestamps = set()
              existing_by_timestamp = {}
              
              for item in existing[data_key]:
                  ts = item['timestamp']
                  # Timestamp'i normalize et
                  if ts.endswith('+00:00'):
                      ts = ts.replace('+00:00', 'Z')
                  elif not ts.endswith('Z'):
                      ts = ts + 'Z'
                  existing_timestamps.add(ts)
                  existing_by_timestamp[ts] = item
              
              print(f"    Mevcut kayÄ±tlar: {len(existing_timestamps)} timestamp")
              
              # Yeni verileri iÅŸle
              merged = existing[data_key].copy()
              changes_made = False
              
              for item in new_items:
                  # Timestamp'i al ve normalize et
                  if isinstance(item['timestamp'], datetime):
                      new_ts = item['timestamp'].astimezone(timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ')
                  else:
                      new_ts = item['timestamp']
                      if new_ts.endswith('+00:00'):
                          new_ts = new_ts.replace('+00:00', 'Z')
                      elif not new_ts.endswith('Z'):
                          new_ts = new_ts + 'Z'
                  
                  count = item['count']
                  uniques = item['uniques']
                  
                  if new_ts in existing_timestamps:
                      existing_item = existing_by_timestamp.get(new_ts)
                      if existing_item:
                          if count > existing_item['count'] or uniques > existing_item['uniques']:
                              # Eski veriyi Ã§Ä±kar
                              merged = [x for x in merged if x['timestamp'] != existing_item['timestamp']]
                              # Yeni veriyi ekle
                              merged.append({
                                  "timestamp": new_ts,
                                  "count": count,
                                  "uniques": uniques
                              })
                              changes_made = True
                              print(f"      ğŸ”„ GÃ¼ncellendi: {new_ts} (count: {count}, uniques: {uniques})")
                  else:
                      # Yeni timestamp
                      merged.append({
                          "timestamp": new_ts,
                          "count": count,
                          "uniques": uniques
                      })
                      changes_made = True
                      print(f"      âœ… Yeni eklendi: {new_ts} (count: {count}, uniques: {uniques})")
              
              if changes_made:
                  merged.sort(key=lambda x: x['timestamp'])
                  total_count = sum(item['count'] for item in merged)
                  total_uniques = sum(item['uniques'] for item in merged)
                  
                  print(f"    âœ… Toplam {len(merged)} kayÄ±t (count: {total_count}, uniques: {total_uniques})")
                  
                  return {
                      data_key: merged,
                      "count": total_count,
                      "uniques": total_uniques
                  }, True
              else:
                  print(f"    âºï¸ DeÄŸiÅŸiklik yok")
                  return existing, False

          try:
              # API baÄŸlantÄ±sÄ±nÄ± test et
              user = g.get_user()
              print(f"âœ… GitHub'a baÄŸlanÄ±ldÄ±: {user.login}")
              
              repos = list(g.get_user().get_repos(type='all'))
              print(f"ğŸ“š Toplam {len(repos)} repo bulundu")
              
              repo_index_path = f"{data_dir}/repo-info.json"
              
              # Mevcut repo index'i oku
              existing_repo_index = []
              if os.path.exists(repo_index_path):
                  with open(repo_index_path, 'r') as f:
                      existing_repo_index = json.load(f)
                  print(f"ğŸ“‹ Mevcut repo index: {len(existing_repo_index)} repo")

              existing_dict = {repo['name']: repo for repo in existing_repo_index}
              updated_repos = []
              
              stats = {
                  'processed': 0,
                  'errors': 0,
                  'new_data': 0,
                  'updated': 0,
                  'no_change': 0
              }

              for repo in repos:
                  if repo.name.startswith('.'):
                      continue
                      
                  repo_dir = f"{data_dir}/repos/{repo.name}"
                  os.makedirs(repo_dir, exist_ok=True)
                  
                  try:
                      print(f"\n{'='*60}")
                      print(f"ğŸ“¦ Repo: {repo.name}")
                      print(f"{'='*60}")
                      
                      repo_changed = False
                      
                      # VIEWS VERÄ°LERÄ°
                      print(f"\n--- VIEWS ---")
                      views_file = f"{repo_dir}/views.json"
                      views_data = {}
                      
                      if os.path.exists(views_file):
                          with open(views_file, 'r') as f:
                              views_data = json.load(f)
                          print(f"ğŸ“ views.json okundu: {len(views_data.get('views', []))} kayÄ±t")
                      
                      try:
                          new_views = repo.get_views_traffic()
                          # API'den gelen veriyi dictionary olarak iÅŸle
                          if isinstance(new_views, dict):
                              views_items = parse_traffic_data(new_views, 'views')
                              if views_items:
                                  merged_views, views_changed = merge_data(views_data, views_items, 'views')
                                  if views_changed:
                                      with open(views_file, 'w') as f:
                                          json.dump(merged_views, f, indent=2)
                                      repo_changed = True
                          else:
                              print(f"    âš ï¸ views: Beklenmeyen veri tipi")
                      except Exception as e:
                          print(f"  âŒ Views hatasÄ±: {str(e)}")
                      
                      # CLONES VERÄ°LERÄ°
                      print(f"\n--- CLONES ---")
                      clones_file = f"{repo_dir}/clones.json"
                      clones_data = {}
                      
                      if os.path.exists(clones_file):
                          with open(clones_file, 'r') as f:
                              clones_data = json.load(f)
                          print(f"ğŸ“ clones.json okundu: {len(clones_data.get('clones', []))} kayÄ±t")
                      
                      for attempt in range(3):
                          try:
                              new_clones = repo.get_clones_traffic()
                              # API'den gelen veriyi dictionary olarak iÅŸle
                              if isinstance(new_clones, dict):
                                  clones_items = parse_traffic_data(new_clones, 'clones')
                                  if clones_items:
                                      merged_clones, clones_changed = merge_data(clones_data, clones_items, 'clones')
                                      if clones_changed:
                                          with open(clones_file, 'w') as f:
                                              json.dump(merged_clones, f, indent=2)
                                          repo_changed = True
                              break
                          except Exception as e:
                              if attempt == 2:
                                  print(f"  âŒ Clones hatasÄ±: {str(e)}")
                              else:
                                  print(f"  âš ï¸ Deneme {attempt+1} baÅŸarÄ±sÄ±z, tekrar deneniyor...")
                                  time.sleep(2)
                      
                      # Repo index'i gÃ¼ncelle
                      existing_data = existing_dict.get(repo.name, {})
                      new_entry = {
                          "name": repo.name,
                          "full_name": repo.full_name,
                          "private": repo.private,
                          "default_branch": repo.default_branch,
                          "homepage": repo.homepage if repo.homepage else "",
                          "updated_at": datetime.now(timezone.utc).isoformat()
                      }
                      
                      for key, value in existing_data.items():
                          if key not in new_entry:
                              new_entry[key] = value
                      
                      updated_repos.append(new_entry)
                      stats['processed'] += 1
                      
                      if repo_changed:
                          if repo.name in existing_dict:
                              stats['updated'] += 1
                              print(f"\nâœ… Repo GÃœNCELLENDÄ°!")
                          else:
                              stats['new_data'] += 1
                              print(f"\nâœ… Yeni repo EKLENDÄ°!")
                      else:
                          stats['no_change'] += 1
                          print(f"\nâºï¸ Repo deÄŸiÅŸmedi")

                      time.sleep(1.2)
                      
                  except Exception as e:
                      print(f"\nâŒ Repo iÅŸlenirken hata: {str(e)}")
                      stats['errors'] += 1
                      if repo.name in existing_dict:
                          updated_repos.append(existing_dict[repo.name])

              # RepolarÄ± sÄ±rala ve kaydet
              updated_repos.sort(key=lambda x: x['name'])
              
              with open(repo_index_path, 'w') as f:
                  json.dump(updated_repos, f, indent=2)
              
              print(f"\n{'='*60}")
              print(f"ğŸ“Š RAPOR")
              print(f"{'='*60}")
              print(f"âœ… Ä°ÅŸlenen repo: {stats['processed']}")
              print(f"âœ¨ Yeni repo: {stats['new_data']}")
              print(f"ğŸ”„ GÃ¼ncellenen: {stats['updated']}")
              print(f"âºï¸ DeÄŸiÅŸmeyen: {stats['no_change']}")
              print(f"âš ï¸ Hata: {stats['errors']}")
              print(f"{'='*60}")

          except Exception as e:
              print(f"ğŸ”¥ KRÄ°TÄ°K HATA: {str(e)}")
              raise
          EOF

      - name: Commit and Push Changes
        run: |
          git config user.name "GitHub Actions"
          git config user.email "actions@github.com"
          
          git add docs/data/
          
          if git diff --quiet && git diff --staged --quiet; then
            echo "â„¹ï¸ No changes to commit"
            exit 0
          fi
          
          git commit -m "ğŸ“ˆ Update traffic data: $(date -u '+%Y-%m-%d %H:%M UTC')"
          
          MAX_RETRIES=5
          for i in $(seq 1 $MAX_RETRIES); do
            echo "ğŸ”„ Push attempt $i of $MAX_RETRIES..."
            
            git pull --rebase origin main && git push origin main && exit 0
            
            if [ $i -eq $MAX_RETRIES ]; then
              echo "âŒ All push attempts failed"
              exit 1
            fi
            
            sleep $((2 ** i))
          done
