name: GitHub Traffic Data Collector

on:
  schedule:
    - cron: '0 * * * *'  # Saatlik √ßalƒ±≈ütƒ±r
  workflow_dispatch:

permissions:
  contents: write

jobs:
  fetch-traffic-data:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.PERSONAL_ACCESS_TOKEN }}
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          echo "pygithub==1.59.0" > requirements.txt
          echo "pyyaml" >> requirements.txt
          pip install -r requirements.txt

      - name: Initialize Data Directory
        run: |
          mkdir -p docs/data/repos
          echo "DATA_DIR=docs/data" >> $GITHUB_ENV

      - name: Collect and Merge Traffic Data
        env:
          GH_TOKEN: ${{ secrets.PERSONAL_ACCESS_TOKEN }}
        run: |
          python - <<EOF
          from github import Github
          import os
          import json
          import time
          from datetime import datetime, timezone

          g = Github(os.environ['GH_TOKEN'])
          data_dir = os.environ['DATA_DIR']
          
          def merge_data(existing, new, key):
              # Mevcut timestamp'leri normalize et ve set olu≈ütur
              existing_timestamps = set()
              for item in existing[key]:
                  ts = item['timestamp']
                  # Z ile bitiyorsa olduƒüu gibi kullan, deƒüilse ekle
                  if not ts.endswith('Z'):
                      ts = ts + 'Z'
                  existing_timestamps.add(ts)
              
              # Mevcut verileri timestamp'e g√∂re indexle (orijinal timestamp'leri koru)
              existing_by_timestamp = {}
              for item in existing[key]:
                  ts_key = item['timestamp']
                  if not ts_key.endswith('Z'):
                      ts_key = ts_key + 'Z'
                  existing_by_timestamp[ts_key] = item
              
              # Yeni verileri ekle/g√ºncelle
              merged = existing[key].copy()
              
              for item in new[key]:
                  # DOƒûRU timestamp formatƒ±
                  new_ts = item.timestamp.astimezone(timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ')
                  
                  # Bu timestamp daha √∂nce var mƒ± kontrol et
                  if new_ts in existing_timestamps:
                      # Varolan veriyi al
                      existing_item = existing_by_timestamp.get(new_ts)
                      if existing_item:
                          # Eƒüer yeni veri daha b√ºy√ºkse g√ºncelle
                          if item.count > existing_item['count'] or item.uniques > existing_item['uniques']:
                              # Eski veriyi √ßƒ±kar
                              merged = [x for x in merged if x['timestamp'] != existing_item['timestamp']]
                              # Yeni veriyi ekle
                              merged.append({
                                  "timestamp": new_ts,
                                  "count": item.count,
                                  "uniques": item.uniques
                              })
                              print(f"  ‚Üª G√ºncellendi: {new_ts} (count: {item.count}, uniques: {item.uniques})")
                      else:
                          print(f"  ‚ö†Ô∏è Timestamp set'te var ama veri bulunamadƒ±: {new_ts}")
                  else:
                      # Yeni timestamp - direkt ekle
                      merged.append({
                          "timestamp": new_ts,
                          "count": item.count,
                          "uniques": item.uniques
                      })
                      print(f"  ‚úö Eklendi: {new_ts} (count: {item.count}, uniques: {item.uniques})")
              
              # Timestamp'e g√∂re sƒ±rala
              merged.sort(key=lambda x: x['timestamp'])
              
              # Toplamlarƒ± g√ºncelle
              total_count = sum(item['count'] for item in merged)
              total_uniques = sum(item['uniques'] for item in merged)
              
              return merged, total_count, total_uniques

          try:
              repos = list(g.get_user().get_repos(type='all'))
              repo_index_path = f"{data_dir}/repo-info.json"
              
              # Mevcut repo index'i oku
              existing_repo_index = []
              if os.path.exists(repo_index_path):
                  with open(repo_index_path, 'r') as f:
                      existing_repo_index = json.load(f)

              # Hƒ±zlƒ± eri≈üim i√ßin dictionary olu≈ütur
              existing_dict = {repo['name']: repo for repo in existing_repo_index}
              updated_repos = []
              
              processed_count = 0
              error_count = 0
              updated_count = 0
              new_count = 0

              for repo in repos:
                  if repo.name.startswith('.'):  # Hidden repo'larƒ± atla
                      continue
                      
                  repo_dir = f"{data_dir}/repos/{repo.name}"
                  os.makedirs(repo_dir, exist_ok=True)
                  
                  try:
                      print(f"\nüì¶ ƒ∞≈üleniyor: {repo.name}")
                      repo_updated = False
                      repo_new = False
                      
                      # Views verilerini i≈üle
                      views_file = f"{repo_dir}/views.json"
                      views_data = {"views": [], "count": 0, "uniques": 0}
                      
                      if os.path.exists(views_file):
                          with open(views_file, 'r') as f:
                              views_data = json.load(f)
                          print(f"  üìä Mevcut views: {len(views_data['views'])} kayƒ±t")
                      
                      try:
                          new_views = repo.get_views_traffic()
                          # API yanƒ±tƒ± s√∂zl√ºk m√º nesne mi kontrol et
                          if isinstance(new_views, dict) and 'views' in new_views:
                              # S√∂zl√ºk ise, i√ßindeki views listesini al ve nesnelere d√∂n√º≈üt√ºr
                              views_items = new_views['views']
                              # Nesne listesi olu≈ütur (her item zaten bir nesne)
                              class ViewObj:
                                  def __init__(self, item):
                                      self.timestamp = item['timestamp'] if isinstance(item['timestamp'], datetime) else datetime.fromisoformat(item['timestamp'].replace('Z', '+00:00'))
                                      self.count = item['count']
                                      self.uniques = item['uniques']
                              view_objects = [ViewObj(item) for item in views_items]
                              
                              # Ge√ßici bir nesne olu≈ütur
                              from types import SimpleNamespace
                              temp_views = SimpleNamespace()
                              temp_views.views = view_objects
                              
                              merged_views, total_count, total_uniques = merge_data(views_data, temp_views, 'views')
                          else:
                              # Normal nesne yapƒ±sƒ±
                              merged_views, total_count, total_uniques = merge_data(views_data, new_views, 'views')
                          
                          # Deƒüi≈üiklik var mƒ± kontrol et
                          if len(merged_views) != len(views_data['views']):
                              repo_new = True
                          else:
                              # ƒ∞√ßerik deƒüi≈ümi≈ü olabilir, detaylƒ± kontrol
                              for i, item in enumerate(merged_views):
                                  if i < len(views_data['views']):
                                      old_item = views_data['views'][i]
                                      if item['count'] != old_item['count'] or item['uniques'] != old_item['uniques']:
                                          repo_updated = True
                                          break
                          
                          views_data['views'] = merged_views
                          views_data['count'] = total_count
                          views_data['uniques'] = total_uniques
                          
                          with open(views_file, 'w') as f:
                              json.dump(views_data, f, indent=2)
                      except Exception as e:
                          print(f"  ‚ö†Ô∏è Views verisi alƒ±namadƒ±: {str(e)}")
                      
                      # Clones verilerini i≈üle
                      clones_file = f"{repo_dir}/clones.json"
                      clones_data = {"clones": [], "count": 0, "uniques": 0}
                      
                      if os.path.exists(clones_file):
                          with open(clones_file, 'r') as f:
                              clones_data = json.load(f)
                          print(f"  üìä Mevcut clones: {len(clones_data['clones'])} kayƒ±t")
                      
                      for attempt in range(3):
                          try:
                              new_clones = repo.get_clones_traffic()
                              
                              # API yanƒ±tƒ± s√∂zl√ºk m√º nesne mi kontrol et
                              if isinstance(new_clones, dict) and 'clones' in new_clones:
                                  # S√∂zl√ºk ise, i√ßindeki clones listesini al ve nesnelere d√∂n√º≈üt√ºr
                                  clones_items = new_clones['clones']
                                  # Nesne listesi olu≈ütur
                                  class CloneObj:
                                      def __init__(self, item):
                                          self.timestamp = item['timestamp'] if isinstance(item['timestamp'], datetime) else datetime.fromisoformat(item['timestamp'].replace('Z', '+00:00'))
                                          self.count = item['count']
                                          self.uniques = item['uniques']
                                  clone_objects = [CloneObj(item) for item in clones_items]
                                  
                                  # Ge√ßici bir nesne olu≈ütur
                                  from types import SimpleNamespace
                                  temp_clones = SimpleNamespace()
                                  temp_clones.clones = clone_objects
                                  
                                  merged_clones, total_count, total_uniques = merge_data(clones_data, temp_clones, 'clones')
                              else:
                                  # Normal nesne yapƒ±sƒ±
                                  merged_clones, total_count, total_uniques = merge_data(clones_data, new_clones, 'clones')
                              
                              # Deƒüi≈üiklik var mƒ± kontrol et
                              if len(merged_clones) != len(clones_data['clones']):
                                  repo_new = True
                              else:
                                  for i, item in enumerate(merged_clones):
                                      if i < len(clones_data['clones']):
                                          old_item = clones_data['clones'][i]
                                          if item['count'] != old_item['count'] or item['uniques'] != old_item['uniques']:
                                              repo_updated = True
                                              break
                              
                              clones_data['clones'] = merged_clones
                              clones_data['count'] = total_count
                              clones_data['uniques'] = total_uniques
                              
                              with open(clones_file, 'w') as f:
                                  json.dump(clones_data, f, indent=2)
                              break
                          except Exception as e:
                              if attempt == 2: 
                                  print(f"  ‚ùå Clone verisi alƒ±namadƒ±: {str(e)}")
                              else:
                                  time.sleep(2)
                      
                      # Repo index'i g√ºncelle
                      existing_data = existing_dict.get(repo.name, {})
                      new_entry = {
                          "name": repo.name,
                          "full_name": repo.full_name,
                          "private": repo.private,
                          "default_branch": repo.default_branch,
                          "homepage": repo.homepage if repo.homepage else "",
                          "updated_at": datetime.now(timezone.utc).isoformat()
                      }
                      
                      # Mevcut √∂zel alanlarƒ± koru
                      for key, value in existing_data.items():
                          if key not in new_entry:
                              new_entry[key] = value
                      
                      updated_repos.append(new_entry)
                      processed_count += 1
                      
                      if repo_new:
                          new_count += 1
                          print(f"  ‚úÖ Yeni veriler eklendi")
                      elif repo_updated:
                          updated_count += 1
                          print(f"  üîÑ Mevcut veriler g√ºncellendi")
                      else:
                          print(f"  ‚è∫Ô∏è Deƒüi≈üiklik yok")

                      time.sleep(1.2)
                      
                  except Exception as e:
                      print(f"  ‚ùå {repo.name} i≈ülenemedi: {str(e)}")
                      error_count += 1
                      if repo.name in existing_dict:
                          updated_repos.append(existing_dict[repo.name])
                      continue

              # Repolarƒ± isme g√∂re sƒ±rala
              updated_repos.sort(key=lambda x: x['name'])
              
              # Nihai repo index'i yaz
              with open(repo_index_path, 'w') as f:
                  json.dump(updated_repos, f, indent=2)
              
              print(f"\n{'='*50}")
              print(f"‚úÖ ƒ∞≈ülem tamamlandƒ±!")
              print(f"üìä ƒ∞≈ülenen repo: {processed_count}")
              print(f"‚ú® Yeni veri eklenen: {new_count}")
              print(f"üîÑ G√ºncellenen: {updated_count}")
              print(f"‚ö†Ô∏è Hata: {error_count}")
              print(f"{'='*50}")

          except Exception as e:
              print(f"üî• Kritik hata: {str(e)}")
              raise
          EOF

      - name: Commit and Push Changes
        run: |
          git config user.name "GitHub Actions"
          git config user.email "actions@github.com"
          
          git add docs/data/
          
          if git diff --quiet && git diff --staged --quiet; then
            echo "‚ÑπÔ∏è No changes to commit"
            exit 0
          fi
          
          git commit -m "üìà Tarihsel veri birikimi: $(date -u '+%Y-%m-%dT%H:%MZ')"
          
          # Retry loop with exponential backoff for conflict resolution
          MAX_RETRIES=5
          for i in $(seq 1 $MAX_RETRIES); do
            echo "üîÑ Push attempt $i of $MAX_RETRIES..."
            
            git pull --rebase origin main && git push origin main && exit 0
            
            if [ $i -eq $MAX_RETRIES ]; then
              echo "‚ùå All push attempts failed"
              exit 1
            fi
            
            WAIT_TIME=$((2 ** i))
            echo "‚ö†Ô∏è Attempt $i failed, waiting ${WAIT_TIME}s before retry..."
            sleep $WAIT_TIME
          done
