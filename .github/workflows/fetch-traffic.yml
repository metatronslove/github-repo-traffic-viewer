name: GitHub Traffic Data Collector

on:
  schedule:
    - cron: '0 * * * *'  # Saatlik Ã§alÄ±ÅŸtÄ±r
  workflow_dispatch:

permissions:
  contents: write

jobs:
  fetch-traffic-data:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.PERSONAL_ACCESS_TOKEN }}
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          echo "pygithub==1.59.0" > requirements.txt
          echo "pyyaml" >> requirements.txt
          pip install -r requirements.txt

      - name: Initialize Data Directory
        run: |
          mkdir -p docs/data/repos
          echo "DATA_DIR=docs/data" >> $GITHUB_ENV

      - name: Collect and Merge Traffic Data
        env:
          GH_TOKEN: ${{ secrets.PERSONAL_ACCESS_TOKEN }}
        run: |
          python - <<EOF
          from github import Github
          import os
          import json
          import time
          from datetime import datetime, timezone
          from collections import defaultdict

          g = Github(os.environ['GH_TOKEN'])
          data_dir = os.environ['DATA_DIR']
          
          def merge_data(existing, new, key):
              existing_timestamps = {item['timestamp'] for item in existing[key]}
              merged = existing[key] + [
                  {
                      "timestamp": item.timestamp.astimezone(timezone.utc).isoformat(),
                      "count": item.count,
                      "uniques": item.uniques
                  } 
                  for item in new[key] 
                  if item.timestamp.astimezone(timezone.utc).isoformat() not in existing_timestamps
              ]
              return sorted(merged, key=lambda x: x['timestamp'])

          try:
              repos = list(g.get_user().get_repos(type='all'))
              repo_index_path = f"{data_dir}/repo-info.json"
              
              # Mevcut repo index'i oku
              existing_repo_index = []
              if os.path.exists(repo_index_path):
                  with open(repo_index_path, 'r') as f:
                      existing_repo_index = json.load(f)

              for repo in repos:
                  repo_dir = f"{data_dir}/repos/{repo.name}"
                  os.makedirs(repo_dir, exist_ok=True)
                  
                  try:
                      # Views verilerini biriktir
                      views_file = f"{repo_dir}/views.json"
                      views_data = {"views": [], "count": 0, "uniques": 0}
                      
                      if os.path.exists(views_file):
                          with open(views_file, 'r') as f:
                              views_data = json.load(f)
                      
                      new_views = repo.get_views_traffic()
                      views_data['views'] = merge_data(views_data, new_views, 'views')
                      views_data['count'] = sum(v['count'] for v in views_data['views'])
                      views_data['uniques'] = sum(v['uniques'] for v in views_data['views'])
                      
                      with open(views_file, 'w') as f:
                          json.dump(views_data, f, indent=2)
                      
                      # Clones verilerini biriktir (3 deneme)
                      clones_file = f"{repo_dir}/clones.json"
                      clones_data = {"clones": [], "count": 0, "uniques": 0}
                      
                      if os.path.exists(clones_file):
                          with open(clones_file, 'r') as f:
                              clones_data = json.load(f)
                      
                      for attempt in range(3):
                          try:
                              new_clones = repo.get_clones_traffic()
                              clones_data['clones'] = merge_data(clones_data, new_clones, 'clones')
                              clones_data['count'] = sum(c['count'] for c in clones_data['clones'])
                              clones_data['uniques'] = sum(c['uniques'] for c in clones_data['clones'])
                              break
                          except Exception as e:
                              if attempt == 2: print(f"âŒ Clone verisi alÄ±namadÄ±: {repo.name}")
                              time.sleep(2)
                      
                      with open(clones_file, 'w') as f:
                          json.dump(clones_data, f, indent=2)

                      # Repo index'i gÃ¼ncelle
                      existing_repo_index = [
                          r for r in existing_repo_index 
                          if r['name'] != repo.name
                      ] + [{
                          "name": repo.name,
                          "full_name": repo.full_name,
                          "private": repo.private,
                          "updated_at": datetime.now(timezone.utc).isoformat()
                      }]

                      time.sleep(1.2)  # Rate limit koruma
                      
                  except Exception as e:
                      print(f"âš ï¸ {repo.name} iÅŸlenemedi: {str(e)}")
                      continue

              # Nihai repo index'i yaz
              with open(repo_index_path, 'w') as f:
                  json.dump(existing_repo_index, f, indent=2)
              
              print(f"âœ… {len(repos)} repo iÅŸlendi")

          except Exception as e:
              print(f"ðŸ”¥ Kritik hata: {str(e)}")
              raise
          EOF

      - name: Commit and Push Changes
        run: |
          git config user.name "GitHub Actions"
          git config user.email "actions@github.com"
          git add docs/data/
          git commit -m "ðŸ“ˆ Tarihsel veri birikimi: $(date -u '+%Y-%m-%dT%H:%MZ')" || echo "DeÄŸiÅŸiklik yok"
          git pull --rebase origin main
          git push origin main