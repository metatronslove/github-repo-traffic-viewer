name: GitHub Traffic Data Collector

on:
  schedule:
    - cron: '0 * * * *'  # Hourly run
  workflow_dispatch:

permissions:
  contents: write

jobs:
  fetch-traffic-data:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.PERSONAL_ACCESS_TOKEN }}
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          echo "pygithub==1.59.0" > requirements.txt
          echo "pyyaml" >> requirements.txt
          pip install -r requirements.txt

      - name: Initialize Data Directory
        run: |
          mkdir -p docs/data/repos
          echo "DATA_DIR=docs/data" >> $GITHUB_ENV

      - name: Collect and Merge Traffic Data
        env:
          GH_TOKEN: ${{ secrets.PERSONAL_ACCESS_TOKEN }}
        run: |
          python - <<EOF
          from github import Github
          import os
          import json
          import time
          from datetime import datetime, timezone

          g = Github(os.environ['GH_TOKEN'])
          data_dir = os.environ['DATA_DIR']
          
          def merge_data(existing, new, key):
              # Normalize timestamps and create lookup dictionaries
              existing_timestamps = set()
              existing_by_timestamp = {}  # Fixed: proper dictionary for lookup
              
              # Process existing items
              for item in existing[key]:
                  ts = item['timestamp']
                  # Normalize timestamp for comparison
                  if ts.endswith('+00:00'):
                      ts = ts.replace('+00:00', 'Z')
                  elif not (ts.endswith('Z') and 'T' in ts):
                      ts = ts.replace('T', 'T') + 'Z'
                  
                  normalized_ts = ts
                  existing_timestamps.add(normalized_ts)
                  existing_by_timestamp[normalized_ts] = item
              
              # Start with all existing items
              merged_dict = {item['timestamp']: item for item in existing[key]}
              
              # Merge new data
              for item in new[key]:
                  ts = item.timestamp.astimezone(timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ')
                  normalized_new_ts = ts
                  
                  if normalized_new_ts in existing_timestamps:
                      # Check if we need to update
                      existing_item = existing_by_timestamp.get(normalized_new_ts)
                      if existing_item and (item.count > existing_item['count'] or item.uniques > existing_item['uniques']):
                          # Update with higher values
                          merged_dict[ts] = {
                              "timestamp": ts,
                              "count": item.count,
                              "uniques": item.uniques
                          }
                  else:
                      # Brand new timestamp
                      merged_dict[ts] = {
                          "timestamp": ts,
                          "count": item.count,
                          "uniques": item.uniques
                      }
              
              # Convert back to list and sort
              merged_list = list(merged_dict.values())
              return sorted(merged_list, key=lambda x: x['timestamp'])

          try:
              repos = list(g.get_user().get_repos(type='all'))
              repo_index_path = f"{data_dir}/repo-info.json"
              
              # Read existing repo index with graceful handling
              existing_repo_index = []
              if os.path.exists(repo_index_path):
                  try:
                      with open(repo_index_path, 'r') as f:
                          existing_repo_index = json.load(f)
                  except:
                      print("‚ö†Ô∏è Could not read existing repo index, starting fresh")
                      existing_repo_index = []
              
              # Create dictionary for quick lookup
              existing_dict = {repo['name']: repo for repo in existing_repo_index}
              updated_repos = []

              for repo in repos:
                  repo_dir = f"{data_dir}/repos/{repo.name}"
                  os.makedirs(repo_dir, exist_ok=True)
                  
                  try:
                      # === VIEWS DATA ===
                      views_file = f"{repo_dir}/views.json"
                      views_data = {"views": [], "count": 0, "uniques": 0}
                      
                      if os.path.exists(views_file):
                          try:
                              with open(views_file, 'r') as f:
                                  views_data = json.load(f)
                          except:
                              print(f"‚ö†Ô∏è Could not read views for {repo.name}, starting fresh")
                      
                      # Get new views from API
                      new_views = repo.get_views_traffic()
                      
                      # Merge old + new (fixed merge function)
                      views_data['views'] = merge_data(views_data, new_views, 'views')
                      views_data['count'] = sum(v['count'] for v in views_data['views'])
                      views_data['uniques'] = sum(v['uniques'] for v in views_data['views'])
                      
                      # Save
                      with open(views_file, 'w') as f:
                          json.dump(views_data, f, indent=2)
                      
                      # === CLONES DATA ===
                      clones_file = f"{repo_dir}/clones.json"
                      clones_data = {"clones": [], "count": 0, "uniques": 0}
                      
                      if os.path.exists(clones_file):
                          try:
                              with open(clones_file, 'r') as f:
                                  clones_data = json.load(f)
                          except:
                              print(f"‚ö†Ô∏è Could not read clones for {repo.name}, starting fresh")
                      
                      # Get new clones from API with retry
                      for attempt in range(3):
                          try:
                              new_clones = repo.get_clones_traffic()
                              clones_data['clones'] = merge_data(clones_data, new_clones, 'clones')
                              clones_data['count'] = sum(c['count'] for c in clones_data['clones'])
                              clones_data['uniques'] = sum(c['uniques'] for c in clones_data['clones'])
                              break
                          except Exception as e:
                              if attempt == 2:
                                  print(f"‚ùå Clone data could not be retrieved: {repo.name}")
                              else:
                                  time.sleep(2)
                      
                      # Save
                      with open(clones_file, 'w') as f:
                          json.dump(clones_data, f, indent=2)

                      # === REPO INFO ===
                      existing_data = existing_dict.get(repo.name, {})
                      
                      # Create new entry with all fields
                      new_entry = {
                          "name": repo.name,
                          "full_name": repo.full_name,
                          "private": repo.private,
                          "default_branch": repo.default_branch,
                          "homepage": repo.homepage if repo.homepage else "",
                          "updated_at": datetime.now(timezone.utc).isoformat()
                      }
                      
                      # Preserve any existing custom fields
                      for key, value in existing_data.items():
                          if key not in new_entry:
                              new_entry[key] = value
                      
                      updated_repos.append(new_entry)
                      
                      # Rate limit protection
                      time.sleep(1.2)
                      
                  except Exception as e:
                      print(f"‚ö†Ô∏è {repo.name} could not be processed: {str(e)}")
                      # Keep existing data if available
                      if repo.name in existing_dict:
                          updated_repos.append(existing_dict[repo.name])
                      continue

              # Sort repos by name for consistency
              updated_repos.sort(key=lambda x: x['name'])
              
              # Write final repo index
              with open(repo_index_path, 'w') as f:
                  json.dump(updated_repos, f, indent=2)
              
              print(f"‚úÖ {len(updated_repos)} repos processed successfully")
              print(f"üìä Total views/clones data preserved historically")

          except Exception as e:
              print(f"üî• Critical error: {str(e)}")
              raise
          EOF

      - name: Commit and Push Changes
        run: |
          git config user.name "GitHub Actions"
          git config user.email "actions@github.com"
          
          git add docs/data/
          
          if git diff --quiet && git diff --staged --quiet; then
            echo "‚ÑπÔ∏è No changes to commit"
            exit 0
          fi
          
          git commit -m "üìà Historical data accumulation: $(date -u '+%Y-%m-%dT%H:%MZ')"
          
          MAX_RETRIES=5
          for i in $(seq 1 $MAX_RETRIES); do
            echo "üîÑ Push attempt $i of $MAX_RETRIES..."
            
            if git pull --rebase origin main; then
              if git push origin main; then
                echo "‚úÖ Successfully pushed after $i attempts"
                exit 0
              fi
            fi
            
            if [ $i -eq $MAX_RETRIES ]; then
              echo "‚ùå All push attempts failed"
              exit 1
            fi
            
            WAIT_TIME=$((2 ** i))
            echo "‚ö†Ô∏è Attempt $i failed, waiting ${WAIT_TIME}s before retry..."
            sleep $WAIT_TIME
            
            git fetch origin main
            git rebase --strategy-option ours origin/main || {
              echo "Rebase failed, trying merge strategy..."
              git merge --strategy-option ours origin/main --no-ff -m "Merge remote changes"
            }
          done
