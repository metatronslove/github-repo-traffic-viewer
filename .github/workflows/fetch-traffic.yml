name: GitHub Traffic Data Collector

on:
  schedule:
    - cron: '0 * * * *'  # Runs hourly
  workflow_dispatch:

jobs:
  fetch-traffic-data:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install pygithub
          python -m pip install pyyaml

      - name: Create data directories
        run: |
          mkdir -p docs/data/repos
          echo "DATA_DIR=docs/data/repos" >> $GITHUB_ENV

      - name: Collect traffic data
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          python - <<EOF
          from github import Github
          import os
          import json
          from datetime import datetime

          g = Github(os.environ['GITHUB_TOKEN'])
          data_dir = os.environ['DATA_DIR']
          
          # Get all repositories (only those accessible with current token)
          repos = list(g.get_user().get_repos())
          
          # Prepare repository index for the HTML page
          repo_index = []
          
          for repo in repos:
              try:
                  repo_dir = f"{data_dir}/{repo.name}"
                  os.makedirs(repo_dir, exist_ok=True)
                  
                  # Save basic repo info for the index
                  repo_index.append({
                      "name": repo.name,
                      "full_name": repo.full_name,
                      "private": repo.private,
                      "updated_at": repo.updated_at.isoformat()
                  })
                  
                  # Get and save traffic data
                  try:
                      views = repo.get_views_traffic()
                      clones = repo.get_clones_traffic()
                      
                      with open(f"{repo_dir}/views.json", "w") as f:
                          json.dump({
                              "views": [{
                                  "timestamp": v.timestamp.isoformat(),
                                  "count": v.count,
                                  "uniques": v.uniques
                              } for v in views['views']],
                              "count": views['count'],
                              "uniques": views['uniques']
                          }, f)
                      
                      with open(f"{repo_dir}/clones.json", "w") as f:
                          json.dump({
                              "clones": [{
                                  "timestamp": c.timestamp.isoformat(),
                                  "count": c.count,
                                  "uniques": c.uniques
                              } for c in clones['clones']],
                              "count": clones['count'],
                              "uniques": clones['uniques']
                          }, f)
                      
                  except Exception as e:
                      print(f"âš ï¸ Couldn't get traffic for {repo.name}: {str(e)}")
                  
                  # Rate limit protection
                  time.sleep(1)
                  
              except Exception as e:
                  print(f"âŒ Error processing {repo.name}: {str(e)}")
          
          # Save repository index (used by the HTML page)
          with open(f"{data_dir}/repo-info.json", "w") as f:
              json.dump(repo_index, f, indent=2)
          
          print(f"âœ… Processed {len(repos)} repositories")
          EOF

      - name: Commit and push changes
        run: |
          git config user.name "GitHub Actions"
          git config user.email "actions@github.com"
          git add docs/data/
          git commit -m "ðŸ“Š Updated traffic data $(date '+%Y-%m-%d %H:%M')" || echo "No changes to commit"
          git push